
\section{Artefact removal} 

\subsection{CCA algorithm}\label{ap1}

\subsubsection{Algorithm CCA-GEP: CCA by Solving a Generalized Eigenvalue Problem (GEP)}

Given the zero-mean multivariate random vectors $x=[x_1(t), ... , x_m(t)]^T$ and $y=[y_1(t), ... , y_n(t)]^T$, with $t=1, ... N$

\textbf{Step 1. }Compute the estimated covariance matrices  ${\hat{C}}_{xx}, {\hat{C}}_{yy}  $ and $ {\hat{C}}_{xy}$. 

\textbf{Step 2.} Solve the following generalized symmetric eigenvalue problem
\begin{equation}
\begin{bmatrix} 
0 & \hat{C}_{xy} \\
\hat{C}^{T}_{xy} & 0 
\end{bmatrix}
\begin{bmatrix} 
\hat{\omega}_x\\
\hat{\omega}_y
\end{bmatrix}=
\begin{bmatrix} 
\hat{C}_{xx} & 0 \\
0 & \hat{C}_{yy} 
\end{bmatrix}
\begin{bmatrix} 
\hat{\omega}_x\\
\hat{\omega}_y
\end{bmatrix}\rho
\end{equation}

subject to $\hat{\omega}^{T}_x{\hat{C}}_{xx}\hat{\omega}_x=1 $ and $ \hat{\omega}^T_y{\hat{C}}_{yy}\hat{\omega}_{y}=1$.

\textbf{Step 3.} Set the canonical coefficients equal to the eigenvalues $\rho$, and the regression weight equal to the corresponding eigenvectors $\hat{\omega}_x $ and $ \hat{\omega}_y$.



\subsubsection{Algorithm CCA-PA: CCA by Computing Principal Angles (PA)}



Given the zero-mean multivariate random vectors $x=[x_1(t), ... , x_m(t)]^T$ and $y=[y_1(t), ... , y_n(t)]^T$, with $t=1, ...  N$.

\textbf{Step 1.} Consider the matrices $\tilde{X}$ and $\tilde{Y}$, defined as follows:
\begin{equation}
\tilde{X}=\begin{bmatrix} 
x_1(1) & \dots & x_m(1) \\
\vdots &  &\vdots\\
x_1(N) & \dots & x_m(N) 
\end{bmatrix},
\tilde{Y}=\begin{bmatrix} 
y_1(1) & \dots & y_n(1) \\
\vdots &  &\vdots\\
y_1(N) & \dots & y_n(N) 
\end{bmatrix}
\end{equation}

\textbf{Step 2.} Compute the QR decompositions of $\tilde{X}$ and $\tilde{Y}$:
\begin{equation}\begin{cases} \tilde{X}=Q_{\tilde{X}}R_{\tilde{X}} \\ \tilde{Y}=Q_{\tilde{Y}}R_{\tilde{Y}}  \end{cases}
\end{equation}
where $Q_{\tilde{X}}$ and  $Q_{\tilde{Y}}$ are orthogonal matrices and $R_{\tilde{X}}$ and  $R_{\tilde{Y}}$ are upper triangular matrices.

\textbf{Step 3.} Compute the Singular Value Decomposition of $Q^T_{\tilde{X}}Q_{\tilde{Y}}$:
\begin{equation}
Q^T_{\tilde{X}}Q_{\tilde{Y}}=US{V^T}
\end{equation}
where $S$ is a diagonal matrix and $U$ and $V$ are orthogonal matrices. The cosines of the principal angles are given by the diagonal elements of $S$.

\textbf{Step 4.} Set the canonical coefficients equal to the diagonal elements of the matrix $S$ and compute the corresponding regression weights as $\omega_{\tilde{X}}={R^{-1}}_{\tilde{X}}U$ and $\omega_{\tilde{Y}}={R^{-1}}_{\tilde{Y}}V$.

The computation of the principal angles yields the most robust implementation of CCA, since it is able to provide reliable results even when the matrices $\tilde{X}$ and $\tilde{Y}$ are singular. 


\subsubsection{ICA}\label{ap3}


To begin with, we shall show the one-unit version of FastICA. By a "unit" we refer to a computational unit,
eventually an artificial neuron, having a weight vector $\texbf{\omega}$ that the neuron is able to update by a learning rule. The
FastICA learning rule finds a direction, i.e. a unit vector $\mathbf{\omega}$ such that the projection $\mathbf{\omega^Tx}$ maximizes nongaussianity.
Nongaussianity is heremeasured by the approximation of negentropy $J\mathbf{(\omega^T x)}$ given in (25). Recall that the variance
of $\mathbf{\omega^T x}$ must here be constrained to unity; for whitened data this is equivalent to constraining the norm of $\omega$ to be
unity.
The FastICA is based on a fixed-point iteration scheme for finding a maximum of the nongaussianity of $\mathbf{\omega^Tx}$,
as measured in (25), see (Hyvärinen and Oja, 1997; Hyvärinen, 1999a). It can be also derived as an approximative
Newton iteration (Hyvärinen, 1999a). Denote by g the derivative of the nonquadratic function $G$ used in (25); for
example the derivatives of the functions in (26) are:
\begin{equation}
g_1(u)=tanh(a_1u), g_2(u)=uexp(-u^2/2)
\end{equation}
where $1\leq$ a $\leq2$ is some suitable constant, often taken as $a_1=1$. The basic form of the FastICA algorithm is as
follows:


1. Choose an initial (e.g. random) weight vector $\mathbf{\omega}$.


2. Let $\omega^+=E\{\mathbf{xg(\omega^T x)}\}-E\{g'(\mathbf{\omega^T x})\}\mathbf{\omega}$

3. Let $\omega=\omega^+/\|\omega^+\|$

4. If not converged, go back to 2. 

Note that convergence means that the old and new values of $\omega$ point in the same direction, i.e. their dot-product is
(almost) equal to 1. It is not necessary that the vector converges to a single point, since $\omega$ and $- \omega$ define the same
direction. This is again because the independent components can be defined only up to a multiplicative sign. Note
also that it is here assumed that the data is prewhitened.
The derivation of FastICA is as follows. First note that the maxima of the approximation of the negentropy of
$\mathbf{\omega^Tx}$ are obtained at certain optima of $E\{G(\mathbf{\omega^Tx})\}$. According to the Kuhn-Tucker conditions (Luenberger, 1969),
the optima of $E\{G(\mathbf{\omega^Tx})\}$ under the constraint $E\{(\mathbf{\omega^Tx})^2\} = \|\omega\|^2 = 1$ are obtained at points where
\begin{equation}
E\{\mathbf{xg(\omega^Tx)}\}-\beta\omega=0 \end{equation}
Let us try to solve this equation by Newton’s method. Denoting the function on the left-hand side of (41) by $F$, we
obtain its Jacobian matrix $JF(\omega)$ as
\begin{equation}
JF(\omega)=E\{\mathbf{xx^Tg'(\omega^Tx)}\}-\beta\mathbf{I} \end{equation}
To simplify the inversion of this matrix, we decide to approximate the first term in (42). Since the data is sphered,
a reasonable approximation seems to be $E\{\mathbf{xx^Tg'(\omega^Tx)}\}\approx E\{\mathbf{xx^T}\}}E\{g'\mathbf{(\omega^Tx)}\}} = E\{g'\mathbf{(\omega^Tx)}\}\mathbf{I}}$. Thus the Jacobian
matrix becomes diagonal, and can easily be inverted. Thus we obtain the following approximative Newton
iteration:
\begin{equation}
\mathbf{\omega^+}=\mathbf{\omega}-[E\{\mathbf{xg(\omega^Tx)}\}-\beta\mathbf{\omega}]/[E\{\mathbf{g'(\omega^Tx)}\}-\beta]
\end{equation}


This algorithm can be further simplified by multiplying both sides of (43) by $\beta-E\{\mathbf{g'(\omega^Tx)}\}$. This gives, after
algebraic simplication, the FastICA iteration.
In practice, the expectations in FastICA must be replaced by their estimates. The natural estimates are of
course the corresponding sample means. Ideally, all the data available should be used, but this is often not a good
idea because the computations may become too demanding. Then the averages can be estimated using a smaller
sample, whose size may have a considerable effect on the accuracy of the final estimates. The sample points should
be chosen separately at every iteration. If the convergence is not satisfactory, one may then increase the sample
size.







\newpage
\subsection{EEG1 BSS  steps of CCA method}\label{CCA1}

\subsubsection{Automated version}


\begin{figure}[!htbp]
\foreach \i in {8,...,22} {%
    \begin{subfigure}[p]{0.33\textwidth}
        \includegraphics[width=0.97\linewidth]{\i}
    \end{subfigure}\quad
}
\end{figure}[!htbp]
\begin{figure}
\foreach \i in {23,...,34} {%
    \begin{subfigure}[p]{0.33\textwidth}
        \includegraphics[width=0.97\linewidth]{\i}
    \end{subfigure}\quad
}
\end{figure}

\newpage
\subsubsection{Manual version}


\begin{figure}[!htbp]
\foreach \i in {700,...,714} {%
    \begin{subfigure}[p]{0.33\textwidth}
        \includegraphics[width=0.97\linewidth]{\i}
    \end{subfigure}\quad
}

\end{figure}[!htbp]
\begin{figure}
\foreach \i in {715,...,720} {%
    \begin{subfigure}[p]{0.33\textwidth}
        \includegraphics[width=0.97\linewidth]{\i}
    \end{subfigure}\quad
}
\end{figure}

\newpage

\subsection{EEG2 BSS steps of CCA method}\label{CCA2}
\subsubsection{Automated version}
\begin{figure}[!htbp]
\foreach \i in {36,...,50} {%
    \begin{subfigure}[p]{0.33\textwidth}
        \includegraphics[width=0.97\linewidth]{\i}
    \end{subfigure}\quad
}
\end{figure}

\begin{figure}[!htbp]
\foreach \i in {51,...,65} {%
    \begin{subfigure}[p]{0.33\textwidth}
        \includegraphics[width=0.97\linewidth]{\i}
    \end{subfigure}\quad
}
\end{figure}

\begin{figure}[!htbp]
\foreach \i in {66,...,80} {%
    \begin{subfigure}[p]{0.33\textwidth}
        \includegraphics[width=0.97\linewidth]{\i}
    \end{subfigure}\quad
}
\end{figure}


\newpage
\subsubsection{Manual version}


\begin{figure}[!htbp]
\foreach \i in {722,...,736} {%
    \begin{subfigure}[p]{0.33\textwidth}
        \includegraphics[width=0.97\linewidth]{\i}
    \end{subfigure}\quad
}

\end{figure}[!htbp]
\begin{figure}
\foreach \i in {737,...,751} {%
    \begin{subfigure}[p]{0.33\textwidth}
        \includegraphics[width=0.97\linewidth]{\i}
    \end{subfigure}\quad
}
\end{figure}

\end{figure}[!htbp]
\begin{figure}
\foreach \i in {752,...,760} {%
    \begin{subfigure}[p]{0.33\textwidth}
        \includegraphics[width=0.97\linewidth]{\i}
    \end{subfigure}\quad
}
\end{figure}

\newpage

\subsection{Appendix for question 1}

Signals are uncorrelated if their covariance is zero:

\begin{equation}\label{equati1}
E(y_{1}y_{2})-E(y_{1})E(y_{2})=0
\end{equation}

Signals are independent if their joined probability is equal the production of the respective probability:

\begin{equation}\label{equati2}
p(y_{1}y_{2})=p(y_{1})p(y_{2})
\end{equation}


Independence implies uncorrelatndess whereas the other way around is not true. 

\subsection{ICA test}\label{Test}


\begin{figure}[!htbp]
\foreach \i in {600,...,605} {%
    \begin{subfigure}[p]{0.5\textwidth}
        \includegraphics[width=0.64\linewidth]{\i}
    \end{subfigure}\quad
}
\end{figure}
